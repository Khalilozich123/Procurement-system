# üì¶ Pipeline Big Data de R√©approvisionnement (Procurement)

Ce projet impl√©mente un pipeline de donn√©es complet ("End-to-End") pour automatiser le r√©approvisionnement d'une cha√Æne de supermarch√©s. Il simule la g√©n√©ration de donn√©es de ventes, leur stockage distribu√© sur un cluster Hadoop, et le calcul des besoins de commande via Trino.

## üöÄ Installation & D√©marrage Rapide

Ce projet est **enti√®rement conteneuris√©** avec Docker. Vous n'avez pas besoin d'installer Python, Java ou Hadoop sur votre machine.

### Pr√©requis
* **Docker Desktop** doit √™tre install√© et en cours d'ex√©cution.

### Instructions de Lancement

Nous avons cr√©√© des scripts d'installation automatique pour simplifier le d√©ploiement.

**Pour Windows :**
1. Ouvrez le dossier du projet.
2. Double-cliquez sur le fichier **`install_project.bat`**.
3. Une fen√™tre s'ouvrira et installera tout automatiquement (d√©marrage des conteneurs, peuplement de la base de donn√©es, configuration de Trino).

## üèóÔ∏è Architecture du Projet

Le pipeline suit une architecture Big Data moderne :

1. **Source de Donn√©es (PostgreSQL) :** Contient les donn√©es de r√©f√©rence ("Master Data") : Produits, Fournisseurs, R√®gles de stock (MOQ, Stock de s√©curit√©).
2. **G√©n√©ration & Ingestion (Python) :** Le script `generate_orders.py` simule 5000 commandes quotidiennes et les t√©l√©verse dans le **Data Lake**.
3. **Stockage Distribu√© (HDFS) :** Un cluster Hadoop avec **3 DataNodes** et un facteur de r√©plication de 3 assure la tol√©rance aux pannes.
4. **Traitement Distribu√© (Trino) :** Moteur de requ√™te SQL distribu√© qui joint les donn√©es brutes (JSON/CSV sur HDFS) avec les donn√©es de r√©f√©rence (PostgreSQL).
5. **Orchestration (Docker) :** Un conteneur d√©di√© (`scheduler`) automatise l'ex√©cution du pipeline chaque jour √† 22h00.

## üõ†Ô∏è D√©pannage (Troubleshooting)

**Probl√®me :** Erreur "NameNode is in Safe Mode".
* **Solution :** Le cluster vient de d√©marrer et v√©rifie l'int√©grit√© des blocs. Attendez 30 secondes ou forcez la sortie :
  `docker exec namenode hdfs dfsadmin -safemode leave`

**Probl√®me :** Les scripts Python √©chouent avec "Connection refused".
* **Solution :** Assurez-vous d'avoir lanc√© le projet via `install_project.bat` pour garantir que les conteneurs sont bien connect√©s au m√™me r√©seau Docker.
