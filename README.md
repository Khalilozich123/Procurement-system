#  Pipeline Big Data de R√©approvisionnement (Procurement)

Ce projet impl√©mente un pipeline de donn√©es complet (**"End-to-End"**) pour automatiser le r√©approvisionnement d'une cha√Æne de supermarch√©s. Il simule la g√©n√©ration de donn√©es de ventes, leur stockage distribu√© sur un cluster Hadoop, et le calcul des besoins de commande via Trino, le tout orchestr√© par **Apache Airflow**.

##  Installation & D√©marrage Rapide

Ce projet est **enti√®rement conteneuris√©** avec Docker. Vous n'avez pas besoin d'installer Python, Java ou Hadoop sur votre machine.

### Pr√©requis
* **Docker Desktop** doit √™tre install√© et en cours d'ex√©cution.

### Instructions de Lancement

Nous avons cr√©√© des scripts d'installation automatique pour simplifier le d√©ploiement et la configuration de l'environnement.

**Pour Windows :**
1. Ouvrez le dossier du projet.
2. Double-cliquez sur le fichier **`installation.bat`**.
3. Une fen√™tre s'ouvrira et installera tout automatiquement (construction de l'image Airflow personnalis√©e, d√©marrage des conteneurs Hadoop/Trino/Postgres, initialisation de la base de donn√©es).

> **Note :** Le premier lancement peut prendre quelques minutes le temps de construire l'image Docker d'Airflow avec les d√©pendances n√©cessaires.

### üåê Acc√®s √† l'Interface de Supervision (Airflow)

Une fois le d√©marrage termin√©, le pipeline est pilotable via l'interface web d'Airflow :

* **URL :** [http://localhost:8081](http://localhost:8081)
* **Identifiant :** `admin`
* **Mot de passe :** `admin`

Vous y trouverez le DAG nomm√© **`supply_chain_pipeline`**. Activez-le (bouton "Unpause" √† gauche) pour lancer l'orchestration des t√¢ches.

##  Architecture du Projet

Le pipeline suit une architecture Big Data moderne divis√©e en 5 couches :

1.  **Source de Donn√©es (PostgreSQL) :** Contient les donn√©es de r√©f√©rence ("Master Data") : Produits, Fournisseurs, R√®gles de stock (MOQ, Stock de s√©curit√©).
2.  **G√©n√©ration & Ingestion (Python) :** Des t√¢ches Airflow simulent 5000 commandes quotidiennes (format JSON) et les t√©l√©versent dans le **Data Lake**.
3.  **Stockage Distribu√© (HDFS) :** Un cluster Hadoop avec **3 DataNodes** et un facteur de r√©plication de 3 assure la tol√©rance aux pannes et le stockage des donn√©es brutes.
4.  **Traitement Distribu√© (Trino) :** Moteur de requ√™te SQL distribu√© qui joint les donn√©es brutes (JSON/CSV sur HDFS) avec les donn√©es de r√©f√©rence (PostgreSQL) "In-Memory" pour calculer les besoins nets.
5.  **Orchestration (Apache Airflow) :** Remplace les scripts s√©quentiels par un **DAG** (Directed Acyclic Graph). Airflow g√®re :
    * L'ordre d'ex√©cution des t√¢ches (G√©n√©ration -> Ingestion -> Calcul).
    * Les reprises automatiques en cas d'√©chec (Retries).
    * L'historique et la centralisation des logs.

##  D√©pannage (Troubleshooting)

**Probl√®me : Erreur "NameNode is in Safe Mode"**
* **Cause :** Le cluster Hadoop vient de d√©marrer et v√©rifie l'int√©grit√© des blocs de donn√©es.
* **Solution :** Attendez 30 secondes ou forcez la sortie du mode sans √©chec avec la commande :
  ```bash
  docker exec namenode hdfs dfsadmin -safemode leave